{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies\n",
        "\n",
        "Necessary imports and installations."
      ],
      "metadata": {
        "id": "443Guu-1rCPz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rQZEIwoSjFnU"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import *\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from collections import defaultdict\n",
        "import re\n",
        "import numpy as np\n",
        "import string\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SShyRuKDjxGf",
        "outputId": "8b3b3697-8e54-4225-ef33-9c5c90aaf5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m33.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n"
          ]
        }
      ],
      "source": [
        "!pip install pyspellchecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4ZeWCSxRjymb"
      },
      "outputs": [],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from spellchecker import SpellChecker"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kWpvXmihj0nC",
        "outputId": "fe6dae97-8978-4738-d40d-03e4aeab345a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the WikiText-103 corpus"
      ],
      "metadata": {
        "id": "WW-tENQYrQWj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bmrcG8dvsCd",
        "outputId": "dfb41638-dfb2-42b0-cb88-92c6b3e3982e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ],
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile(\"./data.zip\", \"r\") as zip:\n",
        "    zip.extractall()\n",
        "    print('Done')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNXjQ_0yvuk9"
      },
      "outputs": [],
      "source": [
        "with open(\"./data/WikiText-103.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    corpus = file.read()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "We break the corpus into sentences and clean each sentence. The cleaned sentences become our context for PPMI."
      ],
      "metadata": {
        "id": "vnlXuU4WrZYj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8JeQiGOkJ14"
      },
      "outputs": [],
      "source": [
        "spell = SpellChecker()\n",
        "def clean_sentences(sentence):\n",
        "    # Remove punctuations and do casefolding\n",
        "    article_no_punctuation = re.sub(r'[^ a-zA-Z]', '', sentence)\n",
        "    word_tokens = word_tokenize(article_no_punctuation.lower())\n",
        "    # Remove Stop Words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_no_stopwords = [word_token for word_token in word_tokens if word_token not in stop_words]\n",
        "    # Keep valid english words and 'unk'\n",
        "    english_words = [filtered_sent for filtered_sent in tokens_no_stopwords if filtered_sent in spell or filtered_sent == 'unk']\n",
        "    normalized_sent = \" \".join(english_words)\n",
        "    return normalized_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3-0LcarlilRJ"
      },
      "outputs": [],
      "source": [
        "sentences = sent_tokenize(corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yj6h4vXhjTln",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e92b186-bd28-4eeb-a4ee-c95773f988bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['  \\n = = Gameplay = = \\n \\n As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces .']\n"
          ]
        }
      ],
      "source": [
        "print(sentences[0:1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DkufvGYsjEc3"
      },
      "outputs": [],
      "source": [
        "cleaned_sentences = [clean_sentences(sentence) for sentence in sentences]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fymeL6f5o1WJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "159ff6ab-bd7d-4005-b1eb-1733063b2e70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['previous unk chronicles games chronicles tactical role playing game players take control military unit take part missions enemy forces', 'stories told comic book like panels animated character portraits characters speaking partially voiced speech bubbles partially unvoiced text']\n"
          ]
        }
      ],
      "source": [
        "print(cleaned_sentences[0:2])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create the Vocabulary for PPMI"
      ],
      "metadata": {
        "id": "Yr284Mw_rvI2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jkCN6U6BjaLC"
      },
      "outputs": [],
      "source": [
        "cleaned_set = \" \".join(cleaned_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iSIdJNzIo8ld"
      },
      "outputs": [],
      "source": [
        "cleaned_set_tokenized = cleaned_set.split()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZpAfH0cEp2cR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a5b4aab-2f27-4a9b-a3f3-400b0dcd190a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['previous', 'unk', 'chronicles', 'games', 'chronicles']\n"
          ]
        }
      ],
      "source": [
        "print(cleaned_set_tokenized[0:5])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CbDpHDUTrM3J"
      },
      "outputs": [],
      "source": [
        "vocab = set(cleaned_set_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aEHCFaParVGV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd895ffa-485f-4617-b4ac-bd58961fa05d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60866\n"
          ]
        }
      ],
      "source": [
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sparse static representation using PPMI.\n",
        "\n",
        "We construct a $k \\times |V|$ PPMI matrix where $k$ is the number of unique words in the input file, $|V|$ is the size of the vocabulary.\n",
        "\n",
        "Acknowledgement: https://stackoverflow.com/questions/58701337/how-to-construct-ppmi-matrix-from-a-text-corpus\n",
        "\n",
        "For multiwords, we take the mean vector. For example, if the word is 'big data', we extract vectors for 'big' and 'data' and take the mean.\n",
        "\n",
        "If a word is an OOV word, we replace assign the PPMI vector of 'unk' to it."
      ],
      "metadata": {
        "id": "Tmz1z9fMr6KK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-DruzZU7nOV"
      },
      "outputs": [],
      "source": [
        "def co_occurrence_count_for_vocab(sentences, vocab, word_list, window_size):\n",
        "    co_occurrences = defaultdict(lambda: defaultdict(int))\n",
        "    total_words = 0\n",
        "    word_freq = defaultdict(int)\n",
        "    # Count co-occurrences and frequencies\n",
        "    for text in sentences:\n",
        "        words = text.lower().split()\n",
        "        total_words += len(words)\n",
        "\n",
        "        for i, token in enumerate(words):\n",
        "            word_freq[token] += 1\n",
        "            if token in vocab:  # Only consider words in vocab\n",
        "                context_words = words[max(0, i-window_size):i] + words[i+1:i+1+window_size]\n",
        "                for context_word in context_words:\n",
        "                    if context_word in vocab:\n",
        "                        co_occurrences[token][context_word] += 1\n",
        "\n",
        "    return co_occurrences, word_freq, total_words\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def compute_ppmi_vector(word, vocab, co_occurrences, word_freq, total_words):\n",
        "    ppmi_vector = []\n",
        "    freq_word = word_freq[word]\n",
        "\n",
        "    for v in vocab:\n",
        "        if v == word:\n",
        "            ppmi_vector.append(0)\n",
        "            continue\n",
        "\n",
        "        co_occurrence_w_v = co_occurrences[word].get(v, 0)\n",
        "        freq_v = word_freq[v]\n",
        "        # 0.9 to prevent log(0).\n",
        "        pmi = np.log2(0.9 + ((co_occurrence_w_v * total_words) /(freq_word * freq_v)))\n",
        "        ppmi_vector.append(max(0, pmi))\n",
        "\n",
        "    return ppmi_vector\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return dot_product / (norm_vec1 * norm_vec2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def ppmi_matrix_for_pairs(sentences, vocab, pairs, window_size):\n",
        "    # Collect all unique words from the pairs\n",
        "    all_words = [word for pair in pairs for phrase in pair for word in phrase.split()]\n",
        "    unique_words = set(all_words)\n",
        "    unique_words.add('unk')\n",
        "    # Co-occurrence counts, word frequencies, and total word count\n",
        "    co_occurrences, word_freq, total_words = co_occurrence_count_for_vocab(sentences, vocab, unique_words, window_size)\n",
        "    # Create a dictionary to store PPMI vectors for each unique word\n",
        "    ppmi_vectors = {}\n",
        "\n",
        "    # Handle unknown words by replacing them with 'unk'\n",
        "    for word in unique_words:\n",
        "        if word not in vocab:\n",
        "            ppmi_vectors[word] = compute_ppmi_vector('unk', vocab, co_occurrences, word_freq, total_words)\n",
        "        else:\n",
        "            ppmi_vectors[word] = compute_ppmi_vector(word, vocab, co_occurrences, word_freq, total_words)\n",
        "\n",
        "    # Calculate cosine similarities for all pairs\n",
        "    similarity_score_list = []\n",
        "\n",
        "    # Handle multi-words by breaking them up and taking the mean vector.\n",
        "    for word1, word2 in pairs:\n",
        "        word1_list = word1.split()\n",
        "        word2_list = word2.split()\n",
        "        ppmi_vector_word1 = np.mean([ppmi_vectors[w] for w in word1_list], axis = 0)\n",
        "        ppmi_vector_word2 = np.mean([ppmi_vectors[w] for w in word2_list], axis = 0)\n",
        "\n",
        "        # Calculate cosine similarity between the two words\n",
        "        cos_sim = cosine_similarity(ppmi_vector_word1, ppmi_vector_word2)\n",
        "        similarity_score_list.append(cos_sim)\n",
        "\n",
        "    return similarity_score_list\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experiments on the test/example file"
      ],
      "metadata": {
        "id": "6aqYLQ5Bs9xn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "xKxeRfJo7-NB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "edd1284b-d36a-4972-a551-4697b95334bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New CSV file with similarity scores saved as ./data/11098060_task1_results.csv\n"
          ]
        }
      ],
      "source": [
        "# Read the CSV file\n",
        "input_csv = './data/CW-1-testdata.csv'\n",
        "df = pd.read_csv(input_csv, header=None)\n",
        "\n",
        "# Extract the values of the first three columns\n",
        "new_df = df.iloc[:, :3].copy()\n",
        "\n",
        "similarity_score_list = []\n",
        "\n",
        "pairs = []\n",
        "\n",
        "for i in range(len(df[1])):\n",
        "    pairs.append([df[1][i], df[2][i]])\n",
        "\n",
        "\n",
        "new_df['similarity_score'] = ppmi_matrix_for_pairs(cleaned_sentences, vocab, pairs, window_size = 2)\n",
        "\n",
        "# Write the new DataFrame to a new CSV file\n",
        "output_csv = './data/11098060_task1_results.csv'\n",
        "new_df.to_csv(output_csv, index=False, header=False)\n",
        "\n",
        "print(f\"New CSV file with similarity scores saved as {output_csv}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}