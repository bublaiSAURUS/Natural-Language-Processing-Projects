{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Dependencies\n",
        "\n",
        "Necessary imports and installations."
      ],
      "metadata": {
        "id": "HfdpJsBGyNfz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XA4-EdP2YQTt"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk import *\n",
        "from gensim.models import Word2Vec\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOL8t-XaZQ1M",
        "outputId": "f29b8e13-df59-45b8-f130-e6ddc3c41402"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyspellchecker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdx3kzJ2EJ5A",
        "outputId": "1231ae02-5d43-4490-ff69-b45322844f0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspellchecker\n",
            "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m41.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyspellchecker\n",
            "Successfully installed pyspellchecker-0.8.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the WikiText-103 corpus"
      ],
      "metadata": {
        "id": "-epeQy3SyW_n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from zipfile import ZipFile\n",
        "with ZipFile(\"./data.zip\", \"r\") as zip:\n",
        "    zip.extractall()\n",
        "    print('Done')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F9IgmICNSvsr",
        "outputId": "27fc480e-ab38-4106-f01d-f44462fb5aa5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Done\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"./data/WikiText-103.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    corpus = file.read()"
      ],
      "metadata": {
        "id": "byx2s7zKV7Rk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preprocessing\n",
        "\n",
        "We break the corpus into sentences and clean each sentence. We train the model on words from each cleaned sentence."
      ],
      "metadata": {
        "id": "Q-zrpkdXyd-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(corpus)"
      ],
      "metadata": {
        "id": "VX4BFjz0x0fm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NwPMKrh_Y253"
      },
      "outputs": [],
      "source": [
        "from spellchecker import SpellChecker\n",
        "spell = SpellChecker()\n",
        "def clean_sentences(sentence):\n",
        "    # Remove punctuations and do case folding\n",
        "    sentence_no_punctuation = re.sub(r'[^ a-zA-Z]', '', sentence)\n",
        "    word_tokens = word_tokenize(sentence_no_punctuation.lower())\n",
        "    # Remove Stop Words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens_no_stopwords = [word_token for word_token in word_tokens if word_token not in stop_words]\n",
        "    # Keep valid english words and 'unk'\n",
        "    english_words = [filtered_sent for filtered_sent in tokens_no_stopwords if filtered_sent in spell or filtered_sent == 'unk']\n",
        "\n",
        "    normalized_sent = \" \".join(english_words)\n",
        "    return normalized_sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "069Io7rnYOq1",
        "outputId": "c629d42c-b51b-40bb-8cf3-5813cd6b312d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1282834\n",
            "['  \\n = = Gameplay = = \\n \\n As with previous <unk> Chronicles games , Valkyria Chronicles III is a tactical role @-@ playing game where players take control of a military unit and take part in missions against enemy forces .', 'Stories are told through comic book @-@ like panels with animated character portraits , with characters speaking partially through voiced speech bubbles and partially through unvoiced text .', 'The player progresses through a series of linear missions , gradually unlocked as maps that can be freely scanned through and replayed as they are unlocked .', \"The route to each story location on the map varies depending on an individual player 's approach : when one option is selected , the other is sealed off to the player .\", 'Outside missions , the player characters rest in a camp , where units can be customized and character growth occurs .', 'Alongside the main story missions are character @-@ specific sub missions relating to different squad members .', \"After the game 's completion , additional episodes are unlocked , some of them having a higher difficulty than those found in the rest of the game .\", \"There are also love simulation elements related to the game 's two main heroines , although they take a very minor role .\", \"The game 's battle system , the <unk> system , is carried over directly from <unk> Chronicles .\", 'During missions , players select each unit using a top @-@ down perspective of the battlefield map : once a character is selected , the player moves the character around the battlefield in third @-@ person .']\n"
          ]
        }
      ],
      "source": [
        "print(len(sentences))\n",
        "print(sentences[0:10])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_sentences = [clean_sentences(sentence) for sentence in sentences]"
      ],
      "metadata": {
        "id": "MJ3vQV0nyJpi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Cleaned Sentences:\", cleaned_sentences[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "blxhjYLoyw9p",
        "outputId": "11ebf823-75b0-4b74-d1ff-6f620b853aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Sentences: ['previous unk chronicles games chronicles tactical role playing game players take control military unit take part missions enemy forces', 'stories told comic book like panels animated character portraits characters speaking partially voiced speech bubbles partially unvoiced text', 'player progresses series linear missions gradually unlocked maps freely scanned replayed unlocked']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_sent = [clean_sentence.split() for clean_sentence in cleaned_sentences]"
      ],
      "metadata": {
        "id": "xkclesAy2GXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Input sentences:', input_sent[0:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9sQsfvZi2N93",
        "outputId": "8f24c95f-543c-4c0a-c7ef-0949808c619c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input sentences: [['previous', 'unk', 'chronicles', 'games', 'chronicles', 'tactical', 'role', 'playing', 'game', 'players', 'take', 'control', 'military', 'unit', 'take', 'part', 'missions', 'enemy', 'forces'], ['stories', 'told', 'comic', 'book', 'like', 'panels', 'animated', 'character', 'portraits', 'characters', 'speaking', 'partially', 'voiced', 'speech', 'bubbles', 'partially', 'unvoiced', 'text'], ['player', 'progresses', 'series', 'linear', 'missions', 'gradually', 'unlocked', 'maps', 'freely', 'scanned', 'replayed', 'unlocked']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dense static representation of words using Word2Vec\n",
        "\n"
      ],
      "metadata": {
        "id": "yOZ1XYjyzgCi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Training\n",
        "\n",
        "We train our model for 10 epochs. For reproducibility, the number of workers has been fixed to 1."
      ],
      "metadata": {
        "id": "0HvGnpw3z_PL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences = input_sent, vector_size = 100, window = 15, min_count=1, workers = 1, epochs = 10)"
      ],
      "metadata": {
        "id": "iYGe6_ayz__3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Vocabulary Size"
      ],
      "metadata": {
        "id": "xZUWZPJ2zYwl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(len(model.wv))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxk9PUoGy3K0",
        "outputId": "38d77c67-d0c0-46e3-9960-c77fbedd78c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "60866\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Method to calculate Cosine Similarity\n",
        "def cosine_similarity(vec1, vec2):\n",
        "    dot_product = np.dot(vec1, vec2)\n",
        "    norm_vec1 = np.linalg.norm(vec1)\n",
        "    norm_vec2 = np.linalg.norm(vec2)\n",
        "\n",
        "    if norm_vec1 == 0 or norm_vec2 == 0:\n",
        "        return 0\n",
        "    else:\n",
        "        return dot_product / (norm_vec1 * norm_vec2)"
      ],
      "metadata": {
        "id": "4aDT1bNnkgiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Handling multiwords and calculating similarity scores\n",
        "\n",
        "\n",
        "For multiwords, we take the mean vector. For example, if the word is 'big data', we extract vectors for 'big' and 'data' and take the mean.\n",
        "\n",
        "If a word is an OOV word, we replace assign the vector for 'unk' to it."
      ],
      "metadata": {
        "id": "R8Vciq7JxvJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def oov_word(word_list):\n",
        "    word_vec = []\n",
        "    #Substitute OOV words by 'unk' else return their vector\n",
        "    for w in word_list:\n",
        "            if w not in model.wv:\n",
        "                word_vec.append(model.wv['unk'])\n",
        "            else:\n",
        "                word_vec.append(model.wv[w])\n",
        "    return word_vec\n",
        "\n",
        "\n",
        "def similarity_scores(pairs):\n",
        "\n",
        "    similarity_score_list = []\n",
        "    for pair in pairs:\n",
        "\n",
        "        word1 = pair[0]\n",
        "        word2 = pair[1]\n",
        "\n",
        "        # Handling multi-words by breaking them up.\n",
        "        word1_list = word1.split()\n",
        "        word2_list = word2.split()\n",
        "\n",
        "        word1_vec = []\n",
        "        word2_vec = []\n",
        "\n",
        "        word1_vec = oov_word(word1_list)\n",
        "        word2_vec = oov_word(word2_list)\n",
        "\n",
        "        embedding_vector1 = np.mean(word1_vec, axis = 0)\n",
        "        embedding_vector2 = np.mean(word2_vec, axis = 0)\n",
        "\n",
        "        score = cosine_similarity(embedding_vector1, embedding_vector2)\n",
        "        similarity_score_list.append(score)\n",
        "\n",
        "\n",
        "    return similarity_score_list"
      ],
      "metadata": {
        "id": "4bq31Y-wvXJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimenting on test/example file."
      ],
      "metadata": {
        "id": "csfWIqAHx_Vw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: Read the CSV file\n",
        "input_csv = './data/CW-1-testdata.csv'\n",
        "df = pd.read_csv(input_csv, header=None)\n",
        "\n",
        "# Step 2: Extract the values of the first three columns\n",
        "new_df = df.iloc[:, :3].copy()\n",
        "similarity_score_list = []\n",
        "\n",
        "\n",
        "pairs = []\n",
        "\n",
        "for i in range(len(df[1])):\n",
        "    pairs.append([df[1][i], df[2][i]])\n",
        "\n",
        "\n",
        "similarity_score_list = similarity_scores(pairs)\n",
        "\n",
        "new_df['similarity_score'] = similarity_score_list\n",
        "\n",
        "# Step 4: Write the new DataFrame to a new CSV file\n",
        "output_csv = './data/11098060_task2_results.csv'\n",
        "new_df.to_csv(output_csv, index=False, header=False)\n",
        "\n",
        "print(f\"New CSV file with similarity scores saved as {output_csv}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JC3aU1Kk6ZD2",
        "outputId": "caa718dc-35e3-4fc0-87f9-2d911ad6aa3b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "New CSV file with similarity scores saved as ./data/11098060_task2_results.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}